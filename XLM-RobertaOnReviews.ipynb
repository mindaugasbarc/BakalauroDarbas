{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XLM-RobertaOnReviews.ipynb","provenance":[{"file_id":"1fJW8EX27dsGOxTSovYHj3LLvpEb8McMZ","timestamp":1590008584031},{"file_id":"1j4TBHSlngJe2EcA_eEAWNnJPoTOgfWA_","timestamp":1589133406144}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPIF1+LwQhl17JhjIVrFInJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lvmzLrU6OsY-","colab_type":"code","outputId":"e3ca86ee-a532-497b-ed4d-f8ff5b315481","executionInfo":{"status":"ok","timestamp":1590008602533,"user_tz":-180,"elapsed":7813,"user":{"displayName":"Mindaugas Barčauskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n","\u001b[K     |████████████████████████████████| 645kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 15.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 21.3MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 47.6MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=66910348c0276ffd0aef3819f7e95de790267dac630cb04ef34bf92d61608f6f\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tHz1HO7PN8C","colab_type":"code","outputId":"193ff334-f9fd-4366-ac66-6dd918542d49","executionInfo":{"status":"ok","timestamp":1590008642401,"user_tz":-180,"elapsed":20625,"user":{"displayName":"Mindaugas Barčauskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5D6hrVzAOxv8","colab_type":"code","outputId":"976993d5-7404-42a1-eb46-00ddf3e13817","executionInfo":{"status":"ok","timestamp":1590011363292,"user_tz":-180,"elapsed":6500,"user":{"displayName":"Mindaugas Barčauskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","\n","import tensorflow as tf\n","import json\n","from typing import List, Optional, Union\n","from transformers import (\n","    XLMRobertaConfig,\n","    XLMRobertaForSequenceClassification,\n","    XLMRobertaTokenizer,\n","    TFXLMRobertaForSequenceClassification,\n","    InputExample,\n","    InputFeatures,\n","    PreTrainedTokenizer\n",")\n","\n","\n","# script parameters\n","BATCH_SIZE = 256\n","EVAL_BATCH_SIZE = BATCH_SIZE\n","USE_XLA = False\n","USE_AMP = False\n","EPOCHS = 3\n","\n","TASK = \"mrpc\"\n","TFDS_TASK = TASK\n","\n","num_labels = 5\n","print(num_labels)\n","\n","tf.config.optimizer.set_jit(USE_XLA)\n","tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\n","\n","# Load tokenizer and model from pretrained model/vocabulary. Specify the number of labels to classify (2+: classification, 1: regression)\n","config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n","tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n","model = TFXLMRobertaForSequenceClassification.from_pretrained(\"jplu/tf-xlm-roberta-base\", config=config)\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    tokenizer: PreTrainedTokenizer,\n","    max_length: Optional[int] = 5,\n","    label_list=(1,2,3,4,5),\n","    output_mode=\"classification\",\n","):\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    def label_from_example(example: InputExample) -> Union[int, float]:\n","        if output_mode == \"classification\":\n","            return label_map[example.label]\n","        elif output_mode == \"regression\":\n","            return float(example.label)\n","        raise KeyError(output_mode)\n","\n","    labels = [label_from_example(example) for example in examples]\n","\n","    batch_encoding = tokenizer.batch_encode_plus(\n","        [(example.text_a, example.text_b) for example in examples], max_length=max_length, pad_to_max_length=True,\n","    )\n","\n","    features = []\n","    for i in range(len(examples)):\n","        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n","\n","        feature = InputFeatures(**inputs, label=labels[i])\n","        features.append(feature)\n","\n","    def gen():\n","        for ex in features:\n","            yield (\n","                {\n","                    \"input_ids\": ex.input_ids,\n","                    \"attention_mask\": ex.attention_mask\n","                },\n","                ex.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None])\n","            },\n","            tf.TensorShape([]),\n","        ))\n","\n","\n","class WordEntry:\n","    def __init__(self, words, result):\n","        self.words = words\n","        self.result = result\n","\n","    def __str__(self):\n","        return ({\"words\": self.words,\n","                 \"result\": self.result\n","                 }).__str__()\n","\n","    def __repr__(self):\n","        return self.__str__()\n","\n","\n","def load_lt_grammar_dataset():\n","    data_file = open(\"/gdrive/My Drive/reviews4_large.txt\", \"r\", encoding=\"utf-8\")\n","    return json.load(data_file)\n","\n","def load_input_examples_from_data(data : list):\n","    data = [WordEntry(x['description'], int(x['rating'])) for x in data]\n","    examples = []\n","    for entry in data:\n","        examples.append(InputExample(\" \".join(entry.words), \" \".join(entry.words), label=entry.result))\n","    return examples\n","\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"geH1xONY7SPY","colab_type":"code","colab":{}},"source":["dataset = load_lt_grammar_dataset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLmfzp7e_w1Z","colab_type":"code","colab":{}},"source":["import random\n","\n","filtered_reviews = []\n","\n","for review in dataset:\n","  if (review['rating'] == '5' and random.randrange(1, 10) > 4):\n","    filtered_reviews.append(review)\n","  elif (review['rating'] != '5'):\n","    filtered_reviews.append(review) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKbl7gVD7dQ4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"87cb4b66-6671-4866-8308-b2e36d842ca9","executionInfo":{"status":"ok","timestamp":1590010860145,"user_tz":-180,"elapsed":693,"user":{"displayName":"Mindaugas Barčauskas","photoUrl":"","userId":"11200784794127443264"}}},"source":["len(filtered_reviews)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2568"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"FSja-CrT6daz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"e692510b-7c64-49e0-a5b2-e726dd5f192a","executionInfo":{"status":"ok","timestamp":1590012207723,"user_tz":-180,"elapsed":282327,"user":{"displayName":"Mindaugas Barčauskas","photoUrl":"","userId":"11200784794127443264"}}},"source":["# Load dataset via TensorFlow Datasets\n","data = load_input_examples_from_data(load_lt_grammar_dataset())\n","train_data = data[0:int(len(data)*0.8)]\n","train_data = train_data[0:BATCH_SIZE*150]\n","valid_data = data[int(len(data)*0.8):]\n","valid_data = valid_data[0:BATCH_SIZE*3]\n","train_examples = len(train_data)\n","\n","# MNLI expects either validation_matched or validation_mismatched\n","valid_examples = len(valid_data)\n","\n","# Prepare dataset for GLUE as a tf.data.Dataset instance\n","train_dataset = convert_examples_to_features(train_data, tokenizer, max_length=5)\n","\n","# MNLI expects either validation_matched or validation_mismatched\n","valid_dataset = convert_examples_to_features(valid_data, tokenizer, max_length=5)\n","train_dataset = train_dataset.shuffle(5).batch(BATCH_SIZE).repeat(-1)\n","valid_dataset = valid_dataset.shuffle(5).batch(EVAL_BATCH_SIZE)\n","\n","# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n","opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n","if USE_AMP:\n","    # loss scaling is currently required when using mixed precision\n","    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n","\n","\n","if num_labels == 1:\n","    loss = tf.keras.losses.MeanSquaredError()\n","else:\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n","model.compile(optimizer=opt, loss=loss, metrics=[metric])\n","\n","# Train and evaluate using tf.keras.Model.fit()\n","train_steps = int(train_examples // BATCH_SIZE)\n","valid_steps = int(valid_examples // EVAL_BATCH_SIZE)\n","print(train_steps)\n","print(valid_steps)\n","\n","\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    steps_per_epoch=train_steps\n",")\n","\n","# Save TF2 model\n","os.makedirs(\"./save/\", exist_ok=True)\n","model.save_pretrained(\"./save/\")\n","\n","# if TASK == \"mrpc\":\n","#     # Load the TensorFlow model in PyTorch for inspection\n","#     # This is to demo the interoperability between the two frameworks, you don't have to\n","#     # do this in real life (you can run the inference on the TF model).\n","#     pytorch_model = BertForSequenceClassification.from_pretrained(\"./save/\", from_tf=True)\n","#\n","#     # Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n","#     sentence_0 = \"This research was consistent with his findings.\"\n","#     sentence_1 = \"His findings were compatible with this research.\"\n","#     sentence_2 = \"His findings were not compatible with this research.\"\n","#     inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors=\"pt\")\n","#     inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors=\"pt\")\n","#\n","#     pred_1 = pytorch_model(**inputs_1)[0].argmax().item()\n","#     pred_2 = pytorch_model(**inputs_2)[0].argmax().item()\n","#     print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n","#     print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")\n"],"execution_count":40,"outputs":[{"output_type":"stream","text":["12\n","3\n","Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification_9/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","12/12 [==============================] - 79s 7s/step - loss: 0.7273 - accuracy: 0.8268\n","Epoch 2/3\n","12/12 [==============================] - 79s 7s/step - loss: 0.6988 - accuracy: 0.8247\n","Epoch 3/3\n","12/12 [==============================] - 78s 6s/step - loss: 0.6758 - accuracy: 0.8287\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QNNsUwZ_gd_4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9at3D9dTeUgu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}