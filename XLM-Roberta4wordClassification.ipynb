{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XLM-Roberta4wordClassification.ipynb","provenance":[{"file_id":"1j4TBHSlngJe2EcA_eEAWNnJPoTOgfWA_","timestamp":1589134730979}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNct4DWLZCYpLE6mBmARLoL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lvmzLrU6OsY-","colab_type":"code","outputId":"e0135fa7-e56b-4acc-98b6-ddd6ad61087b","executionInfo":{"status":"ok","timestamp":1589137435063,"user_tz":-180,"elapsed":4833,"user":{"displayName":"Mindaugas Bar훾auskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":308}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tHz1HO7PN8C","colab_type":"code","outputId":"e015ca75-f335-4a48-f0c6-9dcb25600191","executionInfo":{"status":"ok","timestamp":1589137484119,"user_tz":-180,"elapsed":628,"user":{"displayName":"Mindaugas Bar훾auskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5D6hrVzAOxv8","colab_type":"code","outputId":"956b71a5-bfe6-4e10-ebae-1cdad4349193","executionInfo":{"status":"error","timestamp":1589636628096,"user_tz":-180,"elapsed":2753,"user":{"displayName":"Mindaugas Bar훾auskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":370}},"source":["import os\n","\n","import tensorflow as tf\n","import json\n","from typing import List, Optional, Union\n","from transformers import (\n","    XLMRobertaConfig,\n","    XLMRobertaForSequenceClassification,\n","    XLMRobertaTokenizer,\n","    TFXLMRobertaForSequenceClassification,\n","    InputExample,\n","    InputFeatures,\n","    PreTrainedTokenizer\n",")\n","\n","\n","# script parameters\n","BATCH_SIZE = 256\n","EVAL_BATCH_SIZE = BATCH_SIZE\n","USE_XLA = False\n","USE_AMP = False\n","EPOCHS = 3\n","\n","TASK = \"mrpc\"\n","TFDS_TASK = TASK\n","\n","num_labels = 2\n","print(num_labels)\n","\n","tf.config.optimizer.set_jit(USE_XLA)\n","tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\n","\n","# Load tokenizer and model from pretrained model/vocabulary. Specify the number of labels to classify (2+: classification, 1: regression)\n","config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n","tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n","model = TFXLMRobertaForSequenceClassification.from_pretrained(\"jplu/tf-xlm-roberta-base\", config=config)\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    tokenizer,\n","    max_length: Optional[int] = 50,\n","    label_list=(0,1),\n","    output_mode=\"classification\",\n","):\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    def label_from_example(example: InputExample) -> Union[int, float]:\n","        if output_mode == \"classification\":\n","            return label_map[example.label]\n","        elif output_mode == \"regression\":\n","            return float(example.label)\n","        raise KeyError(output_mode)\n","\n","    labels = [label_from_example(example) for example in examples]\n","\n","    batch_encoding = tokenizer.batch_encode_plus(\n","        [(example.text_a, example.text_b) for example in examples], max_length=max_length, pad_to_max_length=True,\n","    )\n","\n","    features = []\n","    for i in range(len(examples)):\n","        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n","        feature = InputFeatures(**inputs, label=labels[i])\n","        features.append(feature)\n","\n","    print(\"some feature examples\")\n","    print(features[0])\n","    print(features[1])\n","    def gen():\n","        for ex in features:\n","            yield (\n","                {\n","                    \"input_ids\": ex.input_ids,\n","                    \"attention_mask\": ex.attention_mask\n","                },\n","                ex.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None])\n","            },\n","            tf.TensorShape([]),\n","        ))\n","\n","\n","class WordEntry:\n","    def __init__(self, words, result):\n","        self.words = words\n","        self.result = result\n","\n","    def __str__(self):\n","        return ({\"words\": self.words,\n","                 \"result\": self.result\n","                 }).__str__()\n","\n","    def __repr__(self):\n","        return self.__str__()\n","\n","\n","def load_lt_grammar_dataset():\n","    data_file = open(\"/gdrive/My Drive/data/wiki_data_4_words.txt\", \"r\", encoding=\"utf-8\")\n","    return json.load(data_file)\n","\n","def load_input_examples_from_data(data : list):\n","    data = [WordEntry(x['words'], x['result']) for x in data]\n","    examples = []\n","    for entry in data:\n","        examples.append(InputExample(\" \".join(entry.words), \" \".join(entry.words), label=entry.result))\n","    return examples\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-670425d05de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mXLMRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mXLMRobertaForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"GVA3k4eV4D0A","colab_type":"code","colab":{}},"source":["data = load_input_examples_from_data(load_lt_grammar_dataset())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5GSVFNC3EgB","colab_type":"code","outputId":"6f2ce7ea-e63a-4a4d-ec96-8b9b1014871b","executionInfo":{"status":"ok","timestamp":1589143831277,"user_tz":-180,"elapsed":70106,"user":{"displayName":"Mindaugas Bar훾auskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# Load dataset via TensorFlow Datasets\n","\n","train_data = data[0:int(len(data)*0.8)]\n","train_data = train_data[0:BATCH_SIZE*300]\n","valid_data = data[int(len(data)*0.8):]\n","valid_data = valid_data[0:BATCH_SIZE*10]\n","train_examples = len(train_data)\n","\n","# MNLI expects either validation_matched or validation_mismatched\n","valid_examples = len(valid_data)\n","\n","# Prepare dataset for GLUE as a tf.data.Dataset instance\n","train_dataset = convert_examples_to_features(train_data, tokenizer, max_length=50)\n","\n","\n","print(train_data[0])\n","print(tokenizer.batch_encode_plus(\n","        [(example.text_a, example.text_b) for example in [train_data[0]]], max_length=50, pad_to_max_length=True,\n","    ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["some feature examples\n","InputFeatures(input_ids=[0, 100429, 145, 2267, 13148, 7, 2512, 4616, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n","InputFeatures(input_ids=[0, 2267, 13148, 7, 2512, 4616, 82127, 56017, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n","InputExample(guid='Biologija yra mokslas apie', text_a='Biologija yra mokslas apie', text_b=None, label=0)\n","{'input_ids': [[0, 100429, 145, 2267, 13148, 7, 2512, 4616, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gIxvplCu3odf","colab_type":"code","outputId":"17b8da84-aea8-4c9b-ed59-2fb50256f272","executionInfo":{"status":"ok","timestamp":1589143832030,"user_tz":-180,"elapsed":738,"user":{"displayName":"Mindaugas Bar훾auskas","photoUrl":"","userId":"11200784794127443264"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# MNLI expects either validation_matched or validation_mismatched\n","valid_dataset = convert_examples_to_features(valid_data, tokenizer, max_length=50)\n","train_dataset = train_dataset.shuffle(2).batch(BATCH_SIZE).repeat(-1)\n","valid_dataset = valid_dataset.shuffle(2).batch(EVAL_BATCH_SIZE)\n","\n","# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n","opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n","if USE_AMP:\n","    # loss scaling is currently required when using mixed precision\n","    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n","\n","\n","if num_labels == 1:\n","    loss = tf.keras.losses.MeanSquaredError()\n","else:\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n","model.compile(optimizer=opt, loss=loss, metrics=[metric])\n","\n","# Train and evaluate using tf.keras.Model.fit()\n","train_steps = int(train_examples // BATCH_SIZE)\n","valid_steps = int(valid_examples // EVAL_BATCH_SIZE)\n","print(train_steps)\n","print(valid_steps)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["some feature examples\n","InputFeatures(input_ids=[0, 249, 379, 50590, 8111, 12917, 6306, 59605, 13, 4462, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n","InputFeatures(input_ids=[0, 8111, 12917, 6306, 59605, 13, 4462, 21, 3911, 19616, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n","300\n","10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9be7-t3f3Hpe","colab_type":"code","outputId":"21696cc0-2d3c-4bdc-aa1a-174bb6ea044d","colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    steps_per_epoch=train_steps\n",")\n","\n","# Save TF2 model\n","os.makedirs(\"./save/\", exist_ok=True)\n","model.save_pretrained(\"./save/\")\n","\n","# if TASK == \"mrpc\":\n","#     # Load the TensorFlow model in PyTorch for inspection\n","#     # This is to demo the interoperability between the two frameworks, you don't have to\n","#     # do this in real life (you can run the inference on the TF model).\n","#     pytorch_model = BertForSequenceClassification.from_pretrained(\"./save/\", from_tf=True)\n","#\n","#     # Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n","#     sentence_0 = \"This research was consistent with his findings.\"\n","#     sentence_1 = \"His findings were compatible with this research.\"\n","#     sentence_2 = \"His findings were not compatible with this research.\"\n","#     inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors=\"pt\")\n","#     inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors=\"pt\")\n","#\n","#     pred_1 = pytorch_model(**inputs_1)[0].argmax().item()\n","#     pred_2 = pytorch_model(**inputs_2)[0].argmax().item()\n","#     print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n","#     print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","288/300 [===========================>..] - ETA: 12:17 - loss: 0.3638 - accuracy: 0.8625"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QNNsUwZ_gd_4","colab_type":"code","colab":{}},"source":["model.predict()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9at3D9dTeUgu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}